{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition - LSTM\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "Created By: Xavier De Carvalho  \r\n",
    "Created On: 12/08/2021  \r\n",
    "Upated By: N/A  \r\n",
    "Updated On: N/A  \r\n",
    "Version: NER0.0.01\r\n",
    "\r\n",
    "### Requirements\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "##### Data Set - `ner_dataset`     \r\n",
    "[Get the data set from Kaggle](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)     \r\n",
    "\r\n",
    "##### Essential Info About Tagged Items:\r\n",
    "- geo = Geographical Entity\r\n",
    "- org = Organization\r\n",
    "- per = Person\r\n",
    "- tim = Time Indicator\r\n",
    "- art = Artifact\r\n",
    "- eve = Event\r\n",
    "- nat = Natural Phenomenon\r\n",
    "\r\n",
    "##### Required Hardware     \r\n",
    "- GPU     \r\n",
    "\r\n",
    "##### Required Python Packages     \r\n",
    "- Numpy\r\n",
    "- Pandas\r\n",
    "- ScikitLearn\r\n",
    "    - Model_Selection\r\n",
    "- Matplotlib     \r\n",
    "    - PyPlot\r\n",
    "- Tensorflow\r\n",
    "\r\n",
    "### Description     \r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "### Example     \r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies If Needed\n",
    "\n",
    "---\n",
    "\n",
    "NOTE: This might not be required if you're running your notebook instance in the cloud! \n",
    "<br><br>\n",
    "Delete the cell below if this is the case..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sys dependency\r\n",
    "# import sys\r\n",
    "# Install dependencies\r\n",
    "# !{sys.executable} -m pip install numpy\r\n",
    "# !{sys.executable} -m pip install matplotlib\r\n",
    "# !{sys.executable} -m pip install pandas\r\n",
    "# !{sys.executable} -m pip install sklearn\r\n",
    "# !{sys.executable} -m pip install tensorflow\r\n",
    "# !{sys.executable} -m pip install livelossplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n",
    "from tensorflow.keras import Model, Input\r\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, SpatialDropout1D, Bidirectional\r\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\r\n",
    "from livelossplot.tf_keras import PlotLossesCallback\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "# Confirm packages have been imported\r\n",
    "print(\"Packages imported!\")\r\n",
    "\r\n",
    "# Create random seed\r\n",
    "np.random.seed(0)\r\n",
    "print(\"Random seed created!\")\r\n",
    "\r\n",
    "# Set pyplot style\r\n",
    "plt.style.use(\"ggplot\")\r\n",
    "print(\"Pyplot style selected!\")\r\n",
    "\r\n",
    "# Tensorflow details\r\n",
    "print(\r\n",
    "    f'''\r\n",
    "    Tensorflow-\r\n",
    "        Tensorflow version:     {tf.__version__}\r\n",
    "        GPU detected:           {tf.config.list_physical_devices('GPU')}\r\n",
    "    '''\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from CSV\r\n",
    "data = pd.read_csv('ner_dataset.csv', encoding='latin1')\r\n",
    "# Fill null values\r\n",
    "data = data.fillna(method='ffill')\r\n",
    "# Show first (n) values in the dataset\r\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show total number of unique words and tags in data set\r\n",
    "print(\r\n",
    "    f'''\r\n",
    "    Totals-\r\n",
    "        Unique words in corpus:     {data['Word'].nunique()}\r\n",
    "        Unique tags in corpus:      {data['Tag'].nunique()}\r\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Padding Token to Words\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a padding token to the end of the vocabulary\r\n",
    "words = list(set(data[\"Word\"].values))\r\n",
    "words.append(\"ENDPAD\")\r\n",
    "# Get number of words in set\r\n",
    "num_words = len(words)\r\n",
    "\r\n",
    "# Create tags set\r\n",
    "tags = list(set(data[\"Tag\"].values))\r\n",
    "# Get number of tags in set\r\n",
    "num_tags = len(tags)\r\n",
    "\r\n",
    "# Verify ENDPAD is appended to Words and that Tags has not changed\r\n",
    "print(\r\n",
    "    f'''\r\n",
    "        Number of words:    {num_words}\r\n",
    "        Number of tags:     {num_tags}\r\n",
    "    '''\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve sentences and their corresponding tags\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve sentences and their corresponding tags\r\n",
    "class SentenceGetter(object):\r\n",
    "    def __init__(self, data):\r\n",
    "        self.n_sent = 1\r\n",
    "        self.data = data\r\n",
    "        agg_func = lambda s: [\r\n",
    "            (w, p, t) for w, p, t in zip(\r\n",
    "                s[\"Word\"].values.tolist(),\r\n",
    "                s[\"POS\"].values.tolist(),\r\n",
    "                s[\"Tag\"].values.tolist()\r\n",
    "            )\r\n",
    "        ]\r\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\r\n",
    "        self.sentences = [s for s in self.grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize getter\r\n",
    "getter = SentenceGetter(data)\r\n",
    "# Get Sentences\r\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first sentence\r\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define mappings between Sentences and Tags\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a unique index to each word\r\n",
    "word2idx = {w: i+1 for i, w in enumerate(words)}\r\n",
    "# Assign a unique index to each tag\r\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate word and tag indexes\r\n",
    "# word2idx, tag2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad Input Sentences\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentences\r\n",
    "plt.hist([len(s) for s in sentences], bins=50)\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the max length\r\n",
    "max_len = 50\r\n",
    "\r\n",
    "# Create feature matrix\r\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentences] # Numerical representation of our words\r\n",
    "X = pad_sequences(\r\n",
    "    maxlen=max_len, \r\n",
    "    sequences=X, \r\n",
    "    padding='post', \r\n",
    "    value=num_words-1\r\n",
    ")\r\n",
    "\r\n",
    "# Create target vector\r\n",
    "y = [[tag2idx[w[2]] for w in s] for s in sentences]\r\n",
    "y = pad_sequences(\r\n",
    "    maxlen=max_len,\r\n",
    "    sequences=y,\r\n",
    "    padding='post',\r\n",
    "    value=tag2idx[\"O\"]\r\n",
    ")\r\n",
    "y = [to_categorical(i, num_classes=num_tags) for i in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train/Test Splits\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets\r\n",
    "x_train, x_test, y_train, y_test = train_test_split(\r\n",
    "    X, \r\n",
    "    y, \r\n",
    "    test_size=1/10, \r\n",
    "    random_state=1\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Bidirectional LSTM Model\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embeddings\r\n",
    "input_word = Input(shape=(max_len,))\r\n",
    "model = Embedding(input_dim=num_words, output_dim=max_len, input_length=max_len)(input_word)\r\n",
    "\r\n",
    "# Spatial dropout layer\r\n",
    "model = SpatialDropout1D(0.1)(model)\r\n",
    "\r\n",
    "# Bidirectional LSTM\r\n",
    "model = Bidirectional(\r\n",
    "    LSTM(\r\n",
    "        units=100, \r\n",
    "        return_sequences=True,\r\n",
    "        recurrent_dropout=0.1\r\n",
    "    )\r\n",
    ")(model)\r\n",
    "\r\n",
    "# Apply dense layer to each time step\r\n",
    "out = TimeDistributed(Dense(num_tags, activation='softmax'))(model)\r\n",
    "\r\n",
    "# Combine layers\r\n",
    "model = Model(input_word, out)\r\n",
    "\r\n",
    "# Model Summary\r\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the LSTM Model\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\r\n",
    "model.compile(\r\n",
    "    optimizer='adam',\r\n",
    "    loss='categorical_crossentropy',\r\n",
    "    metrics=['accuracy']\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set early stopping\r\n",
    "early_stopping = EarlyStopping(\r\n",
    "    monitor='val_accuracy',\r\n",
    "    patience=1, # Increase this value with higher epochs\r\n",
    "    verbose=0,\r\n",
    "    mode='max',\r\n",
    "    restore_best_weights=False\r\n",
    ") # Stop early if the model is not improving with each new epoch\r\n",
    "\r\n",
    "# Create callbacks list\r\n",
    "callbacks = [PlotLossesCallback(), early_stopping, ] # PlotLossesCallback lets us view the model updates live in the notebook\r\n",
    "\r\n",
    "# Start training the model\r\n",
    "history = model.fit(\r\n",
    "    x_train,\r\n",
    "    np.array(y_train),\r\n",
    "    validation_split=2/10,\r\n",
    "    batch_size=32, # Can increase this when using more powerful GPUs\r\n",
    "    epochs=3, # Avoid hardcoding this unless you need a fast output to test\r\n",
    "    verbose=1,\r\n",
    "    callbacks=callbacks\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on unbiased dataset\r\n",
    "model.evaluate(x_test, np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random data set\r\n",
    "i = np.random.randint(0, x_test.shape[0]) # Random index to get values from test set\r\n",
    "p = model.predict(np.array([x_test[i]])) # Model predictions in One-hot encoded matrix\r\n",
    "p = np.argmax(p, axis=-1)\r\n",
    "\r\n",
    "# Get True Values\r\n",
    "y_true = np.argmax(np.array(y_test), axis=-1)[i]\r\n",
    "\r\n",
    "# Create validation table |...words...|...prediction...|...actual...|\r\n",
    "print(\r\n",
    "    \"{:15}{:5}\\t {}\\n\".format(\"Word\", \"True\", \"Pred\")\r\n",
    ")\r\n",
    "print(\"-\"*30)\r\n",
    "for w, true, pred in zip(x_test[i], y_true, p[0]):\r\n",
    "    print(\"{:15}{:5}\\t {}\\n\".format(words[w-1], tags[true], tags[pred]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1a05dc623ad24f4a74001fceb05ae0d4f7cc7768e50c19a632bece4017063430"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}